{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79a953b",
   "metadata": {},
   "source": [
    "Pavan Kusampudi\n",
    "\n",
    "700762366"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6820b9",
   "metadata": {},
   "source": [
    "### Q5: Evaluation Metrics from a Multi-Class Confusion Matrix\n",
    "\n",
    "The system classified 90 animals into Cat, Dog, or Rabbit. The results are shown below:\n",
    "\n",
    "System \\ Gold\tCat\tDog\tRabbit\n",
    "\n",
    "Cat\t5\t10\t5\n",
    "\n",
    "Dog\t15\t20\t10\n",
    "\n",
    "Rabbit\t0\t15\t10\n",
    "\n",
    "3.\tProgramming Implementation\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "1.\tAccepts the confusion matrix above as input.\n",
    "2.\tComputes per-class precision and recall.\n",
    "3.\tComputes macro-averaged and micro-averaged precision and recall.\n",
    "4.\tPrints all results clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e003867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Given Confusion matrix for 3 classes: Cat, Dog, Rabbit. Where rows are predicted labels and columns are true labels.\n",
    "confusion_matrix = np.array([\n",
    "    [5, 10, 5],   \n",
    "    [15, 20, 10], \n",
    "    [0, 15, 10]   \n",
    "])\n",
    "\n",
    "classes = [\"Cat\", \"Dog\", \"Rabbit\"]  # Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d6adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat: Precision = 0.250, Recall = 0.250\n",
      "Dog: Precision = 0.444, Recall = 0.444\n",
      "Rabbit: Precision = 0.400, Recall = 0.400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Computes per-class precision & recall\n",
    "row_sums = confusion_matrix.sum(axis=1) \n",
    "col_sums = confusion_matrix.sum(axis=0) \n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    TP = confusion_matrix[i, i]\n",
    "    FP = row_sums[i] - TP\n",
    "    FN = col_sums[i] - TP\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n",
    "    print(f\"{label}: Precision = {precision:.3f}, Recall = {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b296286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro-averaged Precision = 0.365\n",
      "Macro-averaged Recall = 0.365\n"
     ]
    }
   ],
   "source": [
    "# 2. Computes macro-averaged precision & recall\n",
    "macro_precision = np.mean(precisions)\n",
    "macro_recall = np.mean(recalls)\n",
    "print(f\"\\nMacro-averaged Precision = {macro_precision:.3f}\")\n",
    "print(f\"Macro-averaged Recall = {macro_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad922fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-averaged Precision = 0.389\n",
      "Micro-averaged Recall = 0.389\n"
     ]
    }
   ],
   "source": [
    "# 3. Computes micro-averaged precision & recall\n",
    "TP_total = np.trace(confusion_matrix)     \n",
    "FP_total = row_sums.sum() - TP_total\n",
    "FN_total = col_sums.sum() - TP_total\n",
    "\n",
    "micro_precision = TP_total / (TP_total + FP_total)\n",
    "micro_recall = TP_total / (TP_total + FN_total)\n",
    "\n",
    "print(f\"\\nMicro-averaged Precision = {micro_precision:.3f}\")\n",
    "print(f\"Micro-averaged Recall = {micro_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c09d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c58f6c60",
   "metadata": {},
   "source": [
    "### Q8. Programming: Bigram Language Model Implementation (based on “Activity: I love NLP corpus” slide)\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Write a Python program to:\n",
    "\n",
    "1.\tRead the training corpus:\n",
    "2.\t<s I love NLP /s>  \n",
    "3.\t<s I love deep learning /s>  \n",
    "4.\t<s deep learning is fun /s>\n",
    "5.\tCompute unigram and bigram counts.\n",
    "6.\tEstimate bigram probabilities using MLE.\n",
    "7.\tImplement a function that calculates the probability of any given sentence.\n",
    "8.\tTest your function on both sentences:\n",
    "o\t<s I love NLP /s>\n",
    "o\t<s I love deep learning </s>\n",
    "9.\tPrint which sentence the model prefers and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58b4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import tee\n",
    "import math\n",
    "\n",
    "corpus = [\n",
    "    [\"<s>\", \"I\", \"love\", \"NLP\", \"</s>\"],\n",
    "    [\"<s>\", \"I\", \"love\", \"deep\", \"learning\", \"</s>\"],\n",
    "    [\"<s>\", \"deep\", \"learning\", \"is\", \"fun\", \"</s>\"],\n",
    "]\n",
    "def bigrams(tokens):\n",
    "    a, b = tee(tokens)\n",
    "    next(b, None)\n",
    "    return list(zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabba31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes unigram and bigram counts\n",
    "unigram_counts = Counter()\n",
    "bigram_counts = Counter()\n",
    "for sent in corpus:\n",
    "    unigram_counts.update(sent)\n",
    "    bigram_counts.update(bigrams(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8261b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram MLE: P(w_i | w_{i-1}) = c(w_{i-1}, w_i) / c(w_{i-1})\n",
    "def bigram_mle_prob(w_prev, w_cur):\n",
    "    c_prev = unigram_counts[w_prev]\n",
    "    c_bigram = bigram_counts.get((w_prev, w_cur), 0)\n",
    "    if c_prev == 0:\n",
    "        return 0.0\n",
    "    return c_bigram / c_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57131f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: pretty table (dict of dicts)\n",
    "cond_probs = {}\n",
    "for (w_prev, w_cur), c in bigram_counts.items():\n",
    "    cond_probs.setdefault(w_prev, {})[w_cur] = c / unigram_counts[w_prev]\n",
    "\n",
    "# 4) Sentence probability (MLE bigram):\n",
    "\n",
    "def sentence_prob(tokens):\n",
    "    logp = 0.0\n",
    "    for w_prev, w_cur in bigrams(tokens):\n",
    "        p = bigram_mle_prob(w_prev, w_cur)\n",
    "        if p == 0.0:\n",
    "            return 0.0, float(\"-inf\")\n",
    "        logp += math.log(p)\n",
    "    return math.exp(logp), logp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb734e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing  two sentences\n",
    "\n",
    "s1 = [\"<s>\", \"I\", \"love\", \"NLP\", \"</s>\"]\n",
    "s2 = [\"<s>\", \"I\", \"love\", \"deep\", \"learning\", \"</s>\"]\n",
    "\n",
    "p1, lp1 = sentence_prob(s1)\n",
    "p2, lp2 = sentence_prob(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0296b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Unigram Counts ------\n",
      "      </s>: 3\n",
      "       <s>: 3\n",
      "         I: 2\n",
      "       NLP: 1\n",
      "      deep: 2\n",
      "       fun: 1\n",
      "        is: 1\n",
      "  learning: 2\n",
      "      love: 2\n"
     ]
    }
   ],
   "source": [
    "# 6) Printing results\n",
    "print(\"------ Unigram Counts ------\")\n",
    "for tok, cnt in sorted(unigram_counts.items()):\n",
    "    print(f\"{tok:>10}: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fef07ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Bigram Counts ------\n",
      "       <s>, I          : 2\n",
      "       <s>, deep       : 1\n",
      "         I, love       : 2\n",
      "       NLP, </s>       : 1\n",
      "      deep, learning   : 2\n",
      "       fun, </s>       : 1\n",
      "        is, fun        : 1\n",
      "  learning, </s>       : 1\n",
      "  learning, is         : 1\n",
      "      love, NLP        : 1\n",
      "      love, deep       : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------ Bigram Counts ------\")\n",
    "for (w_prev, w_cur), cnt in sorted(bigram_counts.items()):\n",
    "    print(f\"{w_prev:>10}, {w_cur:<10} : {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bd88be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Bigram Probabilities (MLE) ------\n",
      "P(I|<s>) = 0.666667\n",
      "P(deep|<s>) = 0.333333\n",
      "P(love|I) = 1.000000\n",
      "P(</s>|NLP) = 1.000000\n",
      "P(learning|deep) = 1.000000\n",
      "P(</s>|fun) = 1.000000\n",
      "P(fun|is) = 1.000000\n",
      "P(</s>|learning) = 0.500000\n",
      "P(is|learning) = 0.500000\n",
      "P(NLP|love) = 0.500000\n",
      "P(deep|love) = 0.500000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n------ Bigram Probabilities (MLE) ------\")\n",
    "for w_prev in sorted(cond_probs):\n",
    "    for w_cur in sorted(cond_probs[w_prev]):\n",
    "        print(f\"P({w_cur}|{w_prev}) = {cond_probs[w_prev][w_cur]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "307fe0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Sentence Probabilities ------\n",
      "S1: <s> I love NLP </s> -> P = 0.333333, logP = -1.098612\n",
      "S2: <s> I love deep learning </s> -> P = 0.166667, logP = -1.791759\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------ Sentence Probabilities ------\")\n",
    "print(f\"S1: {' '.join(s1)} -> P = {p1:.6f}, logP = {lp1:.6f}\")\n",
    "print(f\"S2: {' '.join(s2)} -> P = {p2:.6f}, logP = {lp2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f118be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preferred Sentence from above: S1.\n",
      "\n",
      " Reason: Here the Probability of S1 = 0.3333.Probability of S2 = 0.1667. Under the bigram MLE model, S1 has the higher probability because the sequence “love → NLP” followed by “NLP → </s>” matches the training data more strongly than “love → deep → learning → </s>”.\n"
     ]
    }
   ],
   "source": [
    "preferred = \"S1\" if p1 > p2 else \"S2\" if p2 > p1 else \"Tie\"\n",
    "print(f\"\\nPreferred Sentence from above: {preferred}.\")\n",
    "print(f\"\\n Reason: Here the Probability of S1 = 0.3333.Probability of S2 = 0.1667. Under the bigram MLE model, S1 has the higher probability because the sequence “love → NLP” followed by “NLP → </s>” matches the training data more strongly than “love → deep → learning → </s>”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a0b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
